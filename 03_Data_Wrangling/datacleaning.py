# -*- coding: utf-8 -*-
"""DataCleaning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-hj3AZ0Fh0i-4P3HW4s4i9KKkat0nUq_
"""

column_mapping = {
    'Customer ID': 'customer_id',
    'Customer Name': 'customer_name',
    'Product Category': 'product_category',
    'Product Name': 'product_name',
    'Order Date': 'order_date',
    'Ship Date': 'ship_date',
    'Sales Amount': 'sales_amount',
    'Quantity Ordered': 'quantity_ordered',
    'Unit Price': 'unit_price',
    'Discount %': 'discount_percent',
    'Profit Margin': 'profit_margin',
    'Region ': 'region',  # Note: trailing space
    ' Country': 'country'  # Note: leading space
}

# Rename columns
df_cleaned = df.rename(columns=column_mapping)

"""Drop Columns/Rows"""

for col in df_cleaned.columns:
    dtype = df_cleaned[col].dtype
    non_null_count = df_cleaned[col].count()
    null_count = df_cleaned[col].isnull().sum()
    df_cleaned = df_cleaned[df_cleaned.isnull().sum(axis=1) < threshold] # keep rows with fewer than threshold  total nulls
    unique_count = df_cleaned[col].nunique()
    print(f"{col}: {dtype} (Non-null: {non_null_count}, Null: {null_count})")

"""Numerical Columns"""

# Convert date to formatted-date
df_cleaned['order_date'].head()
df_cleaned['order_date'] = pd.to_datetime(df_cleaned['order_date'], format='%Y-%m-%d', errors='coerce')
df_cleaned['order_date'] = pd.to_datetime(date_series, errors='coerce')

# Convert numerical colums to numeric
df_cleaned['sales_amount'] = pd.to_numeric(df_cleaned['sales_amount'], errors='coerce')
df_cleaned['quantity_ordered'] = pd.to_numeric(df_cleaned['quantity_ordered'], errors='coerce').astype('Int64')
df_cleaned['unit_price'] = pd.to_numeric(df_cleaned['unit_price'], errors='coerce')
df_cleaned['discount_percent'] = pd.to_numeric(df_cleaned['discount_percent'], errors='coerce')
df_cleaned['profit_margin'] = pd.to_numeric(df_cleaned['profit_margin'], errors='coerce')

"""Categorical Columns"""

categorical_columns = ['customer_name', 'product_category', 'product_name', 'region', 'country']
for col in categorical_columns:
    if col in df_cleaned.columns:
        df_cleaned[col] = df_cleaned[col].astype('category')
        print(f"Converted {col} to category")
print(df_cleaned.dtypes)

# Remove rows with all NaN values
count = len(df_cleaned)
df_cleaned = df_cleaned.dropna(how='all')

# Remove rows with too many null values (e.g., more than 50% null)
threshold = len(df_cleaned.columns) * 0.5  # 50% threshold
df_cleaned = df_cleaned.dropna(thresh=len(df_cleaned.columns) - threshold)

# rows_removed = initial_row_count - final_row_count

"""Removing Duplicates"""

exact_duplicates = df_cleaned.duplicated().sum()
if exact_duplicates > 0:
    print("Removing exact duplicates...")
    df_cleaned = df_cleaned.drop_duplicates()

if 'customer_id' in df_cleaned.columns and 'order_date' in df_cleaned.columns:
    key_duplicates = df_cleaned.duplicated(subset=['customer_id', 'order_date']).sum()
    if key_duplicates > 0:
      duplicated_records = df_cleaned[df_cleaned.duplicated(subset=['customer_id', 'order_date'], keep=False)]

# Save cleaned dataset
output_filename = 'cleaned_sales_data.csv'
df_cleaned.to_csv(output_filename, index=False)
print(f"Cleaned dataset saved as: {output_filename}")

# Create a summary of changes made
changes_summary = {
    'original_shape': df.shape,
    'final_shape': df_cleaned.shape,
    'columns_renamed': len(column_mapping),
    'columns_removed': len(columns_to_remove) if columns_to_remove else 0,
    'rows_removed': initial_row_count - len(df_cleaned),
    'data_types_converted': len([col for col in df_cleaned.columns if df_cleaned[col].dtype != df[col].dtype])
}

for key, value in changes_summary.items():
    print(f"{key.replace('_', ' ').title()}: {value}")

"""Ensure positive values"""

# Ensure positive values where appropriate
df['age'] = np.abs(df['age'])
df['income'] = np.abs(df['income'])
df['purchase_amount'] = np.abs(df['purchase_amount'])
df['years_customer'] = np.abs(df['years_customer'])

"""Missing values imputation"""

missing_counts = df.isnull().sum()
columns_with_missing = df.columns[df.isnull().any()].tolist()

# Create a copy of the original dataset for mean imputation
df_imputed = df.copy()

# Identify numerical columns with missing values
numerical_cols_with_missing = df_imputed.select_dtypes(include=[np.number]).columns[df_mean_imputed.select_dtypes(include=[np.number]).isnull().any()].tolist()

# Apply mean imputation
for col in numerical_cols_with_missing:
    mean_value = df_imputed[col].mean()
    median_value = df_imputed[col].median()
    df_mean_imputed[col].fillna(mean_value, inplace=True)
    print(f"Imputed {col} with mean value: {mean_value:.2f}")

# Create visualization comparing imputation methods
fig, axes = plt.subplots(2, 3, figsize=(15, 10))

columns_to_plot = ['age', 'income', 'satisfaction_score']

for i, col in enumerate(columns_to_plot):
    # Original data (with missing values removed for plotting)
    axes[0, i].hist(df[col].dropna(), bins=30, alpha=0.7, label='Original', color='blue')
    axes[0, i].set_title(f'{col} - Original Data')
    axes[0, i].set_xlabel(col)
    axes[0, i].set_ylabel('Frequency')

    # Mean vs Median imputation comparison
    axes[1, i].hist(df_mean_imputed[col], bins=30, alpha=0.5, label='Mean Imputed', color='red')
    axes[1, i].hist(df_median_imputed[col], bins=30, alpha=0.5, label='Median Imputed', color='green')
    axes[1, i].set_title(f'{col} - Imputation Comparison')
    axes[1, i].set_xlabel(col)
    axes[1, i].set_ylabel('Frequency')
    axes[1, i].legend()

plt.tight_layout()
plt.show()

""" Outliers Handling"""

## Create box plots to visualize outliers
fig, axes = plt.subplots(2, 3, figsize=(15, 10))
numerical_columns = ['age', 'income', 'purchase_amount', 'satisfaction_score', 'years_customer']

for i, col in enumerate(numerical_columns):
    row = i // 3
    col_idx = i % 3
    if row < 2 and col_idx < 3:
        axes[row, col_idx].boxplot(df_clean[col])
        axes[row, col_idx].set_title(f'Box Plot - {col}')
        axes[row, col_idx].set_ylabel(col)

# Remove empty subplot
if len(numerical_columns) < 6:
    fig.delaxes(axes[1, 2])

plt.tight_layout()
plt.show()

## IQR Method for Outlier Detection
def detect_outliers_iqr(data, column):
    """
    Detect outliers using the IQR method
    """
    Q1 = data[column].quantile(0.25)
    Q3 = data[column].quantile(0.75)
    IQR = Q3 - Q1

    # Define outlier bounds
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # Identify outliers
    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]

    return outliers, lower_bound, upper_bound

print("=== IQR Method Outlier Detection ===")

outlier_summary = {}
for col in numerical_columns:
    outliers, lower_bound, upper_bound = detect_outliers_iqr(df_clean, col)
    outlier_count = len(outliers)
    outlier_percentage = (outlier_count / len(df_clean)) * 100

    outlier_summary[col] = {
        'count': outlier_count,
        'percentage': outlier_percentage,
        'lower_bound': lower_bound,
        'upper_bound': upper_bound
    }

    print(f"\n{col}:")
    print(f"  Outliers: {outlier_count} ({outlier_percentage:.2f}%)")
    print(f"  Lower bound: {lower_bound:.2f}")
    print(f"  Upper bound: {upper_bound:.2f}")

    if outlier_count > 0:
        print(f"  Outlier range: {outliers[col].min():.2f} to {outliers[col].max():.2f}")

## Z-Score Method for Outlier Detection
def detect_outliers_zscore(data, column, threshold=3):
    """
    Detect outliers using the z-score method
    """
    z_scores = np.abs(stats.zscore(data[column]))
    outliers = data[z_scores > threshold]
    return outliers, z_scores

print("=== Z-Score Method Outlier Detection ===")

z_score_summary = {}
for col in numerical_columns:
    outliers, z_scores = detect_outliers_zscore(df_clean, col)
    outlier_count = len(outliers)
    outlier_percentage = (outlier_count / len(df_clean)) * 100

    z_score_summary[col] = {
        'count': outlier_count,
        'percentage': outlier_percentage,
        'max_z_score': z_scores.max()
    }

    print(f"\n{col}:")
    print(f"  Outliers (|z-score| > 3): {outlier_count} ({outlier_percentage:.2f}%)")
    print(f"  Maximum z-score: {z_scores.max():.2f}")

# Compare IQR vs Z-score methods
print("\n=== Comparison: IQR vs Z-Score Methods ===")
comparison_df = pd.DataFrame({
    'IQR_Count': [outlier_summary[col]['count'] for col in numerical_columns],
    'IQR_Percentage': [outlier_summary[col]['percentage'] for col in numerical_columns],
    'ZScore_Count': [z_score_summary[col]['count'] for col in numerical_columns],
    'ZScore_Percentage': [z_score_summary[col]['percentage'] for col in numerical_columns]
}, index=numerical_columns)

print(comparison_df)

## Apply IQR capping to selected columns
def cap_outliers_iqr(data, column):
    """
    Cap outliers using the IQR method
    """
    Q1 = data[column].quantile(0.25)
    Q3 = data[column].quantile(0.75)
    IQR = Q3 - Q1

    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # Cap the outliers
    data_capped = data.copy()
    data_capped[column] = data_capped[column].clip(lower=lower_bound, upper=upper_bound)

    return data_capped, lower_bound, upper_bound

df_iqr_capped = df_clean.copy()
columns_to_cap = ['age', 'income']  # Focus on columns with significant outliers

print("=== IQR Outlier Capping ===")

for col in columns_to_cap:
    original_stats = df_clean[col].describe()
    df_iqr_capped, lower_bound, upper_bound = cap_outliers_iqr(df_iqr_capped, col)
    capped_stats = df_iqr_capped[col].describe()

    print(f"\n{col} - Before and After Capping:")
    print(f"  Original range: {original_stats['min']:.2f} to {original_stats['max']:.2f}")
    print(f"  Capped range: {capped_stats['min']:.2f} to {capped_stats['max']:.2f}")
    print(f"  Capping bounds: {lower_bound:.2f} to {upper_bound:.2f}")

    # Count how many values were capped
    capped_lower = (df_clean[col] < lower_bound).sum()
    capped_upper = (df_clean[col] > upper_bound).sum()
    print(f"  Values capped: {capped_lower + capped_upper} ({capped_lower} lower, {capped_upper} upper)")

    ## Capping Outliers Using Z-Score Method
    def cap_outliers_zscore(data, column, threshold=3):
    """
    Cap outliers using the z-score method
    """
    mean_val = data[column].mean()
    std_val = data[column].std()

    lower_bound = mean_val - threshold * std_val
    upper_bound = mean_val + threshold * std_val

    # Cap the outliers
    data_capped = data.copy()
    data_capped[column] = data_capped[column].clip(lower=lower_bound, upper=upper_bound)

    return data_capped, lower_bound, upper_bound

# Apply Z-score capping
df_zscore_capped = df_clean.copy()

print("=== Z-Score Outlier Capping ===")

for col in columns_to_cap:
    original_stats = df_clean[col].describe()
    df_zscore_capped, lower_bound, upper_bound = cap_outliers_zscore(df_zscore_capped, col)
    capped_stats = df_zscore_capped[col].describe()

    print(f"\n{col} - Before and After Z-Score Capping:")
    print(f"  Original range: {original_stats['min']:.2f} to {original_stats['max']:.2f}")
    print(f"  Capped range: {capped_stats['min']:.2f} to {capped_stats['max']:.2f}")
    print(f"  Capping# Create comprehensive comparison visualization
fig, axes = plt.subplots(3, 2, figsize=(15, 12))

for i, col in enumerate(columns_to_cap):
    # Original data
    axes[0, i].hist(df_clean[col], bins=30, alpha=0.7, color='blue', label='Original')
    axes[0, i].set_title(f'{col} - Original Data')
    axes[0, i].set_xlabel(col)
    axes[0, i].set_ylabel('Frequency')

    # IQR capped data
    axes[1, i].hist(df_iqr_capped[col], bins=30, alpha=0.7, color='red', label='IQR Capped')
    axes[1, i].set_title(f'{col} - IQR Capped')
    axes[1, i].set_xlabel(col)
    axes[1, i].set_ylabel('Frequency')

    # Z-score capped data
    axes[2, i].hist(df_zscore_capped[col], bins=30, alpha=0.7, color='green', label='Z-Score Capped')
    axes[2, i].set_title(f'{col} - Z-Score Capped')
    axes[2, i].set_xlabel(col)
    axes[2, i].set_ylabel('Frequency')

plt.tight_layout()
plt.show()

# Statistical comparison
print("=== Statistical Comparison of Outlier Treatment Methods ===")
for col in columns_to_cap:
    comparison_stats = pd.DataFrame({
        'Original': df_clean[col].describe(),
        'IQR_Capped': df_iqr_capped[col].describe(),
        'ZScore_Capped': df_zscore_capped[col].describe()
    }) bounds: {lower_bound:.2f} to {upper_bound:.2f}")

    # Count how many values were capped
    capped_lower = (df_clean[col] < lower_bound).sum()
    capped_upper = (df_clean[col] > upper_bound).sum()
    print(f"  Values capped: {capped_lower + capped_upper} ({capped_lower} lower, {capped_upper} upper)")

# Create comprehensive comparison visualization
fig, axes = plt.subplots(3, 2, figsize=(15, 12))

for i, col in enumerate(columns_to_cap):
    # Original data
    axes[0, i].hist(df_clean[col], bins=30, alpha=0.7, color='blue', label='Original')
    axes[0, i].set_title(f'{col} - Original Data')
    axes[0, i].set_xlabel(col)
    axes[0, i].set_ylabel('Frequency')

    # IQR capped data
    axes[1, i].hist(df_iqr_capped[col], bins=30, alpha=0.7, color='red', label='IQR Capped')
    axes[1, i].set_title(f'{col} - IQR Capped')
    axes[1, i].set_xlabel(col)
    axes[1, i].set_ylabel('Frequency')

    # Z-score capped data
    axes[2, i].hist(df_zscore_capped[col], bins=30, alpha=0.7, color='green', label='Z-Score Capped')
    axes[2, i].set_title(f'{col} - Z-Score Capped')
    axes[2, i].set_xlabel(col)
    axes[2, i].set_ylabel('Frequency')

plt.tight_layout()
plt.show()

# Statistical comparison
print("=== Statistical Comparison of Outlier Treatment Methods ===")
for col in columns_to_cap:
    comparison_stats = pd.DataFrame({
        'Original': df_clean[col].describe(),
        'IQR_Capped': df_iqr_capped[col].describe(),
        'ZScore_Capped': df_zscore_capped[col].describe()
    })
 print(comparison_stats)

def comprehensive_data_cleaning(data, missing_strategy='median', outlier_method='iqr', outlier_threshold=3):
    """
    Comprehensive data cleaning function

    Parameters:
    - data: Input DataFrame
    - missing_strategy: 'mean' or 'median' for imputation
    - outlier_method: 'iqr' or 'zscore' for outlier detection
    - outlier_threshold: threshold for z-score method
    """

    print("=== Starting Comprehensive Data Cleaning ===")

    # Step 1: Create a copy of the data
    cleaned_data = data.copy()

    # Step 2: Handle missing values
    print(f"\n1. Handling missing values using {missing_strategy} imputation...")
    numerical_cols = cleaned_data.select_dtypes(include=[np.number]).columns

    for col in numerical_cols:
        if cleaned_data[col].isnull().any():
            if missing_strategy == 'mean':
                fill_value = cleaned_data[col].mean()
            else:  # median
                fill_value = cleaned_data[col].median()

            cleaned_data[col].fillna(fill_value, inplace=True)
            print(f"   Imputed {col} with {missing_strategy}: {fill_value:.2f}")

    # Step 3: Handle outliers
    print(f"\n2. Handling outliers using {outlier_method} method...")

    for col in numerical_cols:
        if col != 'customer_id':  # Skip ID columns
            if outlier_method == 'iqr':
                Q1 = cleaned_data[col].quantile(0.25)
                Q3 = cleaned_data[col].quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR
            else:  # zscore
                mean_val = cleaned_data[col].mean()
                std_val = cleaned_data[col].std()
                lower_bound = mean_val - outlier_threshold * std_val
                upper_bound = mean_val + outlier_threshold * std_val

            # Count outliers before capping
            outliers_before = ((cleaned_data[col] < lower_bound) |
                             (cleaned_data[col] > upper_bound)).sum()

            # Cap outliers
            cleaned_data[col] = cleaned_data[col].clip(lower=lower_bound, upper=upper_bound)

            if outliers_before > 0:
                print(f"   Capped {outliers_before} outliers in {col}")

    # Step 4: Generate cleaning report
    print("\n3. Cleaning Summary:")
    print(f"   Original shape: {data.shape}")
    print(f"   Cleaned shape: {cleaned_data.shape}")
    print(f"   Missing values removed: {data.isnull().sum().sum()}")
    print(f"   Data types: {cleaned_data.dtypes.value_counts().to_dict()}")

    return cleaned_data

# Apply comprehensive cleaning
df_final_cleaned = comprehensive_data_cleaning(
    df,
    missing_strategy='median',
    outlier_method='iqr'
)

# Verify the cleaning
print("\n=== Final Data Quality Check ===")
print("Missing values in cleaned data:")
print(df_final_cleaned.isnull().sum())
print("\nFinal dataset statistics:")
print(df_final_cleaned.describe())

"""Data Quality Assessment"""

def assess_data_quality(original_data, cleaned_data):
    """
    Assess the quality improvements after data cleaning
    """

    print("=== Data Quality Assessment ===")

    # Missing value comparison
    original_missing = original_data.isnull().sum().sum()
    cleaned_missing = cleaned_data.isnull().sum().sum()

    print(f"\n1. Missing Values:")
    print(f"   Original: {original_missing}")
    print(f"   Cleaned: {cleaned_missing}")
    print(f"   Improvement: {original_missing - cleaned_missing} missing values handled")

    # Statistical stability comparison
    print(f"\n2. Statistical Stability:")
    numerical_cols = original_data.select_dtypes(include=[np.number]).columns

    for col in numerical_cols:
        if col != 'customer_id':
            original_mean = original_data[col].mean()
            cleaned_mean = cleaned_data[col].mean()
            original_std = original_data[col].std()
            cleaned_std = cleaned_data[col].std()

            print(f"\n   {col}:")
            print(f"     Mean: {original_mean:.2f} → {cleaned_mean:.2f}")
            print(f"     Std:  {original_std:.2f} → {cleaned_std:.2f}")
            print(f"     Std reduction: {((original_std - cleaned_std) / original_std * 100):.1f}%")

    # Data completeness
    original_completeness = (1 - original_data.isnull().sum() / len(original_data)) * 100
    cleaned_completeness = (1 - cleaned_data.isnull().sum() / len(cleaned_data)) * 100

    print(f"\n3. Data Completeness by Column:")
    completeness_comparison = pd.DataFrame({
        'Original_Completeness_%': original_completeness,
        'Cleaned_Completeness_%': cleaned_completeness,
        'Improvement_%': cleaned_completeness - original_completeness
    })
    print(completeness_comparison)

    return completeness_comparison

# Perform quality assessment
quality_report = assess_data_quality(df, df_final_cleaned)

# Create before/after comparison visualization
fig, axes = plt.subplots(2, 3, figsize=(18, 12))

columns_to_visualize = ['age', 'income', 'purchase_amount', 'satisfaction_score', 'years_customer']

for i, col in enumerate(columns_to_visualize):
    row = i // 3
    col_idx = i % 3

    if row < 2 and col_idx < 3:
        # Original data (excluding missing values for visualization)
        axes[row, col_idx].hist(df[col].dropna(), bins=30, alpha=0.6,
                               label='Original', color='red', density=True)

        # Cleaned data
        axes[row, col_idx].hist(df_final_cleaned[col], bins=30, alpha=0.6,
                               label='Cleaned', color='blue', density=True)

        axes[row, col_idx].set_title(f'{col} - Before vs After Cleaning')
        axes[row, col_idx].set_xlabel(col)
        axes[row, col_idx].set_ylabel('Density')
        axes[row, col_idx].legend()

# Remove empty subplot
if len(columns_to_visualize) == 5:
    fig.delaxes(axes[1, 2])

plt.tight_layout()
plt.show()

# Export cleaned dataset
output_filename = 'cleaned_customer_data.csv'
df_final_cleaned.to_csv(output_filename, index=False)
print(f"\nCleaned dataset exported to: {output_filename}")

# Final summary
print("\n=== Final Data Cleaning Summary ===")
print(f"Original dataset: {df.shape[0]} rows, {df.shape[1]} columns")
print(f"Cleaned dataset: {df_final_cleaned.shape[0]} rows, {df_final_cleaned.shape[1]} columns")
print(f"Missing values handled: {df.isnull().sum().sum()}")
print(f"Data quality score: {(df_final_cleaned.isnull().sum().sum() == 0) * 100}% complete")

# Display sample of cleaned data
print("\nSample of cleaned data:")
print(df_final_cleaned.head(10))

"""Memory Optimization"""

def optimize_memory_usage(df):
  for col in df.columns:
        col_type = df[col].dtype

        if col_type != 'object' and col_type.name != 'category':
            c_min = df[col].min()
            c_max = df[col].max()

            if str(col_type)[:3] == 'int':
                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                    df[col] = df[col].astype(np.int8)
                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                    df[col] = df[col].astype(np.int16)
                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                    df[col] = df[col].astype(np.int32)

            elif str(col_type)[:5] == 'float':
                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:
                    df[col] = df[col].astype(np.float32)

    end_mem = df.memory_usage(deep=True).sum() / 1024**2
    print(f'Memory usage decreased from {start_mem:.2f} MB to {end_mem:.2f} MB ({100 * (start_mem - end_mem) / start_mem:.1f}% reduction)')

    return df

df_cleaned = optimize_memory_usage(df_cleaned)

